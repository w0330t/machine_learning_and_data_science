# Optimization Using Gradient Descent: Linear Regression

In this assignment, you will build a simple linear regression model to predict sales based on TV marketing expenses. You will investigate three different approaches to this problem. You will use `NumPy` and `Scikit-Learn` linear regression models, as well as construct and optimize the sum of squares cost function with gradient descent from scratch.

%%
åœ¨è¿™ä¸ªä½œä¸šä¸­ï¼Œä½ å°†æ„å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶åŸºäºç”µè§†è¥é”€çš„è´¹ç”¨é¢„æµ‹é”€å”®é¢ã€‚
ä½ å°†ä½¿ç”¨ä¸‰ç§ä¸åŒçš„æ–¹æ³•æ¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ã€‚
ä½ å°†ä½¿ç”¨ `NumPy` å’Œ `Scikit-Learn` çº¿æ€§å›å½’æ¨¡å‹ï¼Œä»¥åŠä»é›¶å¼€å§‹æ„å»ºå’Œä¼˜åŒ–æ¢¯åº¦ä¸‹é™çš„å¹³æ–¹å’Œæˆæœ¬å‡½æ•°ã€‚
%%

# Table of Contents

- [ 1 - Open the Dataset and State the Problem](#1)
  - [ Exercise 1](#ex01)
- [ 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`](#2)
  - [ 2.1 - Linear Regression with `NumPy`](#2.1)
    - [ Exercise 2](#ex02)
  - [ 2.2 - Linear Regression with `Scikit-Learn`](#2.2)
    - [ Exercise 3](#ex03)
    - [ Exercise 4](#ex04)
- [ 3 - Linear Regression using Gradient Descent](#3)
  - [ Exercise 5](#ex05)
  - [ Exercise 6](#ex06)

## Packages

Load the required packages:


```python
import numpy as np
# A library for programmatic plot generation.
import matplotlib.pyplot as plt
# A library for data manipulation and analysis.
import pandas as pd
# LinearRegression from sklearn.
from sklearn.linear_model import LinearRegression
```

<a name='1'></a>
## 1 - Open the Dataset and State the Problem

In this lab, you will build a linear regression model for a simple [Kaggle dataset](https://www.kaggle.com/code/devzohaib/simple-linear-regression/notebook), saved in a file `data/tvmarketing.csv`. The dataset has only two fields: TV marketing expenses (`TV`) and sales amount (`Sales`).

%%
åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œä½ å°†ä¸ºä¸€ä¸ªç®€å•çš„ [Kaggle æ•°æ®é›†](https://www.kaggle.com/code/devzohaib/simple-linear-regression/notebook) æ„å»ºä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œå®ƒä¿å­˜åœ¨æ–‡ä»¶ `data/tvmarketing.csv` ä¸­ã€‚
è¿™ä¸ªæ•°æ®é›†ä»…åŒ…å«ä¸¤ä¸ªæ•°åˆ—ï¼šç”µè§†è¥é”€è´¹ç”¨ (`TV`) å’Œé”€é‡ (`Sales`)ã€‚
%%

<a name='ex01'></a>
### Exercise 1

Use `pandas` function `pd.read_csv` to open the .csv file the from the `path`.


```python
path = "data/tvmarketing.csv"

### START CODE HERE ### (~ 1 line of code)
adv = pd.read_csv(path)
### END CODE HERE ###
```


```python
# Print some part of the dataset.
adv.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TV</th>
      <th>Sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>230.1</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>44.5</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17.2</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>151.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>180.8</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div>



##### __Expected Output__ 

```Python
	TV	Sales
0	230.1	22.1
1	44.5	10.4
2	17.2	9.3
3	151.5	18.5
4	180.8	12.9
```

`pandas` has a function to make plots from the DataFrame fields. By default, matplotlib is used at the backend. Let's use it here:

%%
`pandas` æœ‰ä¸€ä¸ªä» DataFrame çš„å­—æ®µä¸­ç”Ÿæˆå›¾åƒçš„å‡½æ•°ã€‚
ä¸€èˆ¬æ¥è¯´ï¼Œmatplotlib ä¼šåœ¨åç«¯è¢«è°ƒç”¨ã€‚
è®©æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å®ƒï¼š
%%


```python
adv.plot(x='TV', y='Sales', kind='scatter', c='black')
```




    <Axes: xlabel='TV', ylabel='Sales'>




    
![png](C2_W2_Assignment_files/C2_W2_Assignment_12_1.png)
    


You can use this dataset to solve a simple problem with linear regression: given a TV marketing budget, predict sales.

%%
ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†è§£å†³ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’é—®é¢˜ï¼šç»™å‡ºç”µè§†è¥é”€é¢„ç®—ï¼Œé¢„æµ‹é”€é‡ã€‚
%%

<a name='2'></a>
## 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`

Save the required field of the DataFrame into variables `X` and `Y`:

%%
ä¿å­˜è¿™ä¸¤ä¸ª DataFrame å­—æ®µåˆ°å˜é‡ X å’Œ Y
%%


```python
X = adv['TV']
Y = adv['Sales']
```

<a name='2.1'></a>
### 2.1 - Linear Regression with `NumPy`

You can use the function `np.polyfit(x, y, deg)` to fit a polynomial of degree `deg` to points $(x, y)$, minimising the sum of squared errors. You can read more in the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). Taking `deg = 1` you can obtain the slope `m` and the intercept `b` of the linear regression line:

%%
æ‚¨å¯ä»¥ä½¿ç”¨å‡½æ•° `np.polyfit(x, y, deg)` æ¥æ‹Ÿåˆä¸€ä¸ª `deg` æ¬¡å¤šé¡¹å¼åˆ°ç‚¹ $(x, y)$ ä¸Šï¼Œæœ€å°åŒ–å¹³æ–¹å’Œçš„è¯¯å·®ã€‚
æ›´å¤šç»†èŠ‚å¯ä»¥å‚è€ƒè¿™ç¯‡[æ–‡æ¡£](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)ã€‚
ä½¿ç”¨ `deg = 1` ä½ å¯ä»¥å¾—åˆ°çº¿æ€§å›å½’çº¿çš„æ–œç‡ `m` å’Œæˆªè· `b`ã€‚
%%


```python
m_numpy, b_numpy = np.polyfit(X, Y, 1)

print(f"Linear regression with NumPy. Slope: {m_numpy}. Intercept: {b_numpy}")
```

    Linear regression with NumPy. Slope: 0.04753664043301975. Intercept: 7.0325935491276965


*Note*: [`NumPy` documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) suggests the [`Polynomial.fit` class method](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit) as recommended for new code as it is more stable numerically. But in this simple example, you can stick to the `np.polyfit` function for simplicity.

%%
*æ³¨æ„*ï¼š
[`NumPy` æ–‡æ¡£](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)  ä¸­æ¨èåœ¨æ–°çš„ä»£ç ä¸­ä½¿ç”¨ [`Polynomial.fit` ç±»æ–¹æ³•](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit)ï¼Œå› ä¸ºåœ¨æ•°å€¼ä¸Šæ›´åŠ ç¨³å®šã€‚
ä½†æ˜¯åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œä¸ºäº†ä¿æŒç®€æ´ï¼Œä½¿ç”¨äº† `np.polyfit` å‡½æ•°ã€‚
%%

You can plot the linear regression line by running the following code. The regression line is red.

%%
ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ç»˜åˆ¶çº¿æ€§å›å½’çº¿ã€‚
è¿™æ ¹çº¿æ˜¯çº¢è‰²çš„ã€‚
%%


```python
def plot_linear_regression(X, Y, x_label, y_label, m, b, X_pred=np.array([]), Y_pred=np.array([])):
    fig, ax = plt.subplots(1,1,figsize=(8,5))
    ax.plot(X, Y, 'o', color='black')
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)

    ax.plot(X, m*X + b, color='red')
    # Plot prediction points (empty arrays by default - the predictions will be calculated later).
    ax.plot(X_pred, Y_pred, 'o', color='blue', markersize=8)
    
plot_linear_regression(X, Y, 'TV', 'Sales', m_numpy, b_numpy)
```


    
![png](C2_W2_Assignment_files/C2_W2_Assignment_22_0.png)
    


<a name='ex02'></a>
### Exercise 2

Make predictions substituting the obtained slope and intercept coefficients into the equation $Y = mX + b$, given an array of $X$ values.

%%
ç”¨ä¹‹å‰è·å¾—çš„ç³»æ•°æ›¿æ¢æ–¹ç¨‹ $Y = mX + b$ æ–œç‡å’Œæˆªè·ï¼Œæ ¹æ®ç»™å®šçš„ X å€¼æ•°ç»„è¿›è¡Œé¢„æµ‹ã€‚
%%


```python
# This is organised as a function only for grading purposes.
def pred_numpy(m, b, X):
    ### START CODE HERE ### (~ 1 line of code)
    Y = m * X + b
    ### END CODE HERE ###

    return Y
```


```python
X_pred = np.array([50, 120, 280])
Y_pred_numpy = pred_numpy(m_numpy, b_numpy, X_pred)

print(f"TV marketing expenses:\n{X_pred}")
print(f"Predictions of sales using NumPy linear regression:\n{Y_pred_numpy}")
```

    TV marketing expenses:
    [ 50 120 280]
    Predictions of sales using NumPy linear regression:
    [ 9.40942557 12.7369904  20.34285287]


##### __Expected Output__ 

```Python
TV marketing expenses:
[ 50 120 280]
Predictions of sales using NumPy linear regression:
[ 9.40942557 12.7369904  20.34285287]
```

Now you can add the prediction points to the plot (blue dots).

%%
ç°åœ¨åœ¨ç»˜åˆ¶çš„å›¾å½¢ä¸­æ·»åŠ é¢„æµ‹ç‚¹ï¼ˆè“è‰²ç‚¹ï¼‰
%%


```python
plot_linear_regression(X, Y, 'TV', 'Sales', m_numpy, b_numpy, X_pred, Y_pred_numpy)
```


    
![png](C2_W2_Assignment_files/C2_W2_Assignment_28_0.png)
    


<a name='2.2'></a>
### 2.2 - Linear Regression with `Scikit-Learn`

`Scikit-Learn` is an open-source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. `Scikit-learn` provides dozens of built-in machine learning algorithms and models, called **estimators**. Each estimator can be fitted to some data using its `fit` method. Full documentation can be found [here](https://scikit-learn.org/stable/).

%%
`Scikit-Learn` æ˜¯ä¸€ä¸ªå¼€æºçš„æœºå™¨å­¦ä¹ åº“ï¼Œå®ƒæ”¯æŒæ”¯æŒç›‘ç£å¼å­¦ä¹ å’Œæ— ç›‘ç£å¼å­¦ä¹ ã€‚
å®ƒåŒæ ·æä¾›å„ç§æ¨¡å‹æ‹Ÿåˆå·¥å…·ï¼Œæ•°æ®é¢„å¤„ç†ï¼Œæ¨¡å‹é€‰æ‹©ï¼Œæ¨¡å‹è¯„ä¼°å’Œç­‰ç­‰å…¶ä»–å·¥å…·ã€‚
`Scikit-learn` å†…ç½®äº†å‡ åä¸ªæœºå™¨å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹ï¼Œå®ƒä»¬è¢«ç§°ä¸ºâ€œä¼°è®¡å™¨â€ã€‚
ä¼°è®¡å™¨ä½¿ç”¨ `fit` æ–¹æ³•æ‹Ÿåˆæ•°æ®ã€‚
å®Œæ•´çš„æ–‡æ¡£å‚è€ƒ[è¿™é‡Œ](https://scikit-learn.org/stable/)ã€‚
%%

Create an estimator object for a linear regression model:

%%
åˆ›å»ºä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹çš„ä¼°è®¡å™¨å¯¹è±¡ã€‚
%%


```python
lr_sklearn = LinearRegression()
```

The estimator can learn from data calling the `fit` function. However, trying to run the following code you will get an error, as the data needs to be reshaped into 2D array:

%%
ä¼°è®¡å™¨å¯ä»¥é€šè¿‡ `fit` å‡½æ•°å­¦ä¹ æ•°æ®ã€‚
æ— è®ºå¦‚ä½•ï¼Œå°è¯•è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œå®ƒæƒ è¿”å›ä¸€ä¸ªé”™è¯¯ï¼Œè¿™æ˜¯å› ä¸ºæ•°æ®éœ€è¦å†²é‡æ–°æ•´ç†æˆäºŒä½æ•°ç»„ã€‚
%%


```python
print(f"Shape of X array: {X.shape}")
print(f"Shape of Y array: {Y.shape}")

try:
    lr_sklearn.fit(X, Y)
except ValueError as err:
    print(err)
```

    Shape of X array: (200,)
    Shape of Y array: (200,)
    Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.


You can increase the dimension of the array by one with `reshape` function, or there is another another way to do it:

%%
å¯ä»¥ä½¿ç”¨ `reshape` å‡½æ•°è®©æ•°ç»„çš„ç»´åº¦åŠ ä¸€ï¼Œæˆ–è€…ç”¨å¦å¤–ä¸€ç§æ–¹æ³•ï¼š
%%


```python
X_sklearn = np.array(X)[:, np.newaxis]
Y_sklearn = np.array(Y)[:, np.newaxis]

print(f"Shape of new X array: {X_sklearn.shape}")
print(f"Shape of new Y array: {Y_sklearn.shape}")
```

    Shape of new X array: (200, 1)
    Shape of new Y array: (200, 1)


<a name='ex03'></a>
### Exercise 3

Fit the linear regression model passing `X_sklearn` and `Y_sklearn` arrays into the function `lr_sklearn.fit`.

%%
é€šè¿‡ `lr_sklearn.fit` å‡½æ•°ï¼Œä¼ å…¥ `X_sklearn` å’Œ `Y_sklearn` ï¼Œæ‹Ÿåˆçº¿æ€§å›å½’æ¨¡å‹ã€‚
%%


```python
### START CODE HERE ### (~ 1 line of code)
lr_sklearn.fit(X_sklearn, Y_sklearn)
### END CODE HERE ###
```




<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "â–¸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  display: none;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  display: block;
  width: 100%;
  overflow: visible;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "â–¾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}

.estimator-table summary {
    padding: .5rem;
    font-family: monospace;
    cursor: pointer;
}

.estimator-table details[open] {
    padding-left: 0.1rem;
    padding-right: 0.1rem;
    padding-bottom: 0.3rem;
}

.estimator-table .parameters-table {
    margin-left: auto !important;
    margin-right: auto !important;
}

.estimator-table .parameters-table tr:nth-child(odd) {
    background-color: #fff;
}

.estimator-table .parameters-table tr:nth-child(even) {
    background-color: #f6f6f6;
}

.estimator-table .parameters-table tr:hover {
    background-color: #e0e0e0;
}

.estimator-table table td {
    border: 1px solid rgba(106, 105, 104, 0.232);
}

.user-set td {
    color:rgb(255, 94, 0);
    text-align: left;
}

.user-set td.value pre {
    color:rgb(255, 94, 0) !important;
    background-color: transparent !important;
}

.default td {
    color: black;
    text-align: left;
}

.user-set td i,
.default td i {
    color: black;
}

.copy-paste-icon {
    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);
    background-repeat: no-repeat;
    background-size: 14px 14px;
    background-position: 0;
    display: inline-block;
    width: 14px;
    height: 14px;
    cursor: pointer;
}
</style><body><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LinearRegression</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted" data-param-prefix="">
        <div class="estimator-table">
            <details>
                <summary>Parameters</summary>
                <table class="parameters-table">
                  <tbody>

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('fit_intercept',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">fit_intercept&nbsp;</td>
            <td class="value">True</td>
        </tr>


        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('copy_X',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">copy_X&nbsp;</td>
            <td class="value">True</td>
        </tr>


        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('tol',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">tol&nbsp;</td>
            <td class="value">1e-06</td>
        </tr>


        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('n_jobs',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">n_jobs&nbsp;</td>
            <td class="value">None</td>
        </tr>


        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('positive',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">positive&nbsp;</td>
            <td class="value">False</td>
        </tr>

                  </tbody>
                </table>
            </details>
        </div>
    </div></div></div></div></div><script>function copyToClipboard(text, element) {
    // Get the parameter prefix from the closest toggleable content
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;

    const originalStyle = element.style;
    const computedStyle = window.getComputedStyle(element);
    const originalWidth = computedStyle.width;
    const originalHTML = element.innerHTML.replace('Copied!', '');

    navigator.clipboard.writeText(fullParamName)
        .then(() => {
            element.style.width = originalWidth;
            element.style.color = 'green';
            element.innerHTML = "Copied!";

            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        })
        .catch(err => {
            console.error('Failed to copy:', err);
            element.style.color = 'red';
            element.innerHTML = "Failed!";
            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        });
    return false;
}

document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const paramName = element.parentElement.nextElementSibling.textContent.trim();
    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;

    element.setAttribute('title', fullParamName);
});
</script></body>




```python
m_sklearn = lr_sklearn.coef_
b_sklearn = lr_sklearn.intercept_

print(f"Linear regression using Scikit-Learn. Slope: {m_sklearn}. Intercept: {b_sklearn}")
```

    Linear regression using Scikit-Learn. Slope: [[0.04753664]]. Intercept: [7.03259355]


##### __Expected Output__ 

```Python
Linear regression using Scikit-Learn. Slope: [[0.04753664]]. Intercept: [7.03259355]
```

Note that you have got the same result as with the `NumPy` function `polyfit`. Now, to make predictions it is convenient to use `Scikit-Learn` function `predict`. 

%%
æ³¨æ„ï¼Œä½ ä¹‹å‰å·²ç»é€šè¿‡ `NumPy` çš„å‡½æ•° `polyfit` è·å¾—äº†ç›¸åŒçš„ç»“æœï¼Œç°åœ¨æƒ³è¦é¢„æµ‹åªéœ€è¦ä½¿ç”¨ `Scikit-Learn` çš„å‡½æ•° `predict`ï¼Œéå¸¸æ–¹ä¾¿ã€‚
%%

<a name='ex04'></a>
### Exercise 4


Increase the dimension of the $X$ array using the function `np.newaxis` (see an example above) and pass the result to the `lr_sklearn.predict` function to make predictions.

%%
ä½¿ç”¨å‡½æ•° `np.newaxis` å¢åŠ  X æ•°ç»„çš„ç»´åº¦ï¼ˆå¯ä»¥çœ‹ä¸Šé¢çš„ç¤ºä¾‹ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªç»“æœä¼ å…¥ `lr_sklearn.predict` å‡½æ•°æ¥è¿›è¡Œé¢„æµ‹ã€‚
%%


```python
# This is organised as a function only for grading purposes.
def pred_sklearn(X, lr_sklearn):
    ### START CODE HERE ### (~ 2 lines of code)
    X_2D = X[:, np.newaxis]
    Y = lr_sklearn.predict(X_2D)
    ### END CODE HERE ###
    
    return Y
```


```python
Y_pred_sklearn = pred_sklearn(X_pred, lr_sklearn)

print(f"TV marketing expenses:\n{X_pred}")
print(f"Predictions of sales using Scikit_Learn linear regression:\n{Y_pred_sklearn.T}")
```

    TV marketing expenses:
    [ 50 120 280]
    Predictions of sales using Scikit_Learn linear regression:
    [[ 9.40942557 12.7369904  20.34285287]]


##### __Expected Output__ 

```Python
TV marketing expenses:
[ 50 120 280]
Predictions of sales using Scikit_Learn linear regression:
[[ 9.40942557 12.7369904  20.34285287]]
```

The predicted values are also the same.

%%
é¢„æµ‹çš„ç»“æœä¹Ÿæ˜¯ç›¸åŒçš„ã€‚
%%

<a name='3'></a>
## 3 - Linear Regression using Gradient Descent

Functions to fit the models automatically are convenient to use, but for an in-depth understanding of the model and the maths behind it is good to implement an algorithm by yourself. Let's try to find linear regression coefficients $m$ and $b$, by minimising the difference between original values $y^{(i)}$ and predicted values $\hat{y}^{(i)}$ with the **loss function** $L\left(w, b\right)  = \frac{1}{2}\left(\hat{y}^{(i)} - y^{(i)}\right)^2$ for each of the training examples. Division by $2$ is taken just for scaling purposes, you will see the reason below, calculating partial derivatives.

%%
å‡½æ•°è‡ªåŠ¨æ‹Ÿåˆäº†æ¨¡å‹ä½¿ç”¨èµ·æ¥éå¸¸çš„æ–¹ä¾¿ï¼Œä½†æ˜¯äº†ä¸ºæ›´æ·±å…¥çš„ç†è§£è¿™ä¸ªæ¨¡å‹ä»¥åŠèƒŒåçš„æ•°å­¦åŸç†ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±å®ç°æ•´ä¸ªç®—æ³•ã€‚
è®©æˆ‘ä»¬å°è¯•å¯»æ‰¾çº¿æ€§å›å½’ç³»æ•° $m$ å’Œ $b$ï¼Œæ–¹æ³•æ˜¯æœ€å°åŒ–æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸­åŸå§‹å€¼ $y^{(i)}$ ä¸é¢„æµ‹å€¼ $\hat{y}^{(i)}$ ä¹‹é—´çš„å·®å€¼ï¼Œé‡‡ç”¨**æŸå¤±å‡½æ•°** $L\left(w, b\right)  = \frac{1}{2}\left(\hat{y}^{(i)} - y^{(i)}\right)^2$ã€‚
ç„¶åä¸ºäº†ä¾¿äºç¼©æ”¾å°†ç»“æœé™¤ä»¥ 2ï¼Œä½ å°†åœ¨ä¸‹é¢çœ‹åˆ°åŸå› ï¼Œè®¡ç®—åå¯¼æ•°ã€‚
%%

To compare the resulting vector of the predictions $\hat{Y}$ with the vector $Y$ of original values $y^{(i)}$, you can take an average of the loss function values for each of the training examples:

%%
å°†é¢„æµ‹å‘é‡ $\hat{Y}$ å’Œå‘é‡ $Y$ çš„åŸå§‹å€¼ $y^{(i)}$ çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œä½ éœ€è¦å¹³å‡åŒ–æ¯ä¸ªæŸå¤±å‡½æ•°è®­ç»ƒæ ·æœ¬çš„å€¼ï¼š
%%

$$E\left(m, b\right) = \frac{1}{2n}\sum_{i=1}^{n} \left(\hat{y}^{(i)} - y^{(i)}\right)^2 = 
\frac{1}{2n}\sum_{i=1}^{n} \left(mx^{(i)}+b - y^{(i)}\right)^2,\tag{1}$$

where $n$ is a number of data points. This function is called the sum of squares **cost function**. To use gradient descent algorithm, calculate partial derivatives as:

%%
å…¶ä¸­ $n$ æ˜¯æ•°æ®ç‚¹çš„æ•°é‡ï¼Œè¿™ä¸ªå‡½æ•°è¢«ç§°ä¸ºè¿™ä¸ªå¹³æ–¹å’Œçš„**æˆæœ¬å‡½æ•°**ã€‚
ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œè®¡ç®—åå¯¼æ•°ï¼š
%%

$$
\begin{align}
\frac{\partial E }{ \partial m } &= 
\frac{1}{n}\sum_{i=1}^{n} \left(mx^{(i)}+b - y^{(i)}\right)x^{(i)},\\
\frac{\partial E }{ \partial b } &= 
\frac{1}{n}\sum_{i=1}^{n} \left(mx^{(i)}+b - y^{(i)}\right),
\tag{2}\end{align}
$$

and update the parameters iteratively using the expressions

%%
ç„¶åä½¿ç”¨ä¸‹é¢çš„è¡¨è¾¾å¼æ›´æ–°è¿­ä»£å‚æ•°
%%

$$
\begin{align}
m &= m - \alpha \frac{\partial E }{ \partial m },\\
b &= b - \alpha \frac{\partial E }{ \partial b },
\tag{3}\end{align}
$$

where $\alpha$ is the learning rate.

%%
å…¶ä¸­ $\alpha$ æ˜¯å­¦ä¹ ç‡ã€‚
%%

Original arrays `X` and `Y` have different units. To make gradient descent algorithm efficient, you need to bring them to the same units. A common approach to it is called **normalization**: substract the mean value of the array from each of the elements in the array and divide them by standard deviation (a statistical measure of the amount of dispersion of a set of values). If you are not familiar with mean and standard deviation, do not worry about this for now - this is covered in the next Course of Specialization.

%%
åŸå§‹æ•°ç»„ `X` å’Œ `Y` çš„å•ä½æ˜¯ä¸åŒçš„ã€‚
è¦è®©æ¢¯åº¦ä¸‹é™ç®—æ³•é«˜æ•ˆçš„è¿è¡Œï¼Œä½ éœ€è¦æŠŠå®ƒä»¬ç»Ÿä¸€åˆ°ç›¸åŒçš„é‡çº²ã€‚
ä¸€ä¸ªé€šå¸¸çš„åšæ³•è¢«ç§°ä¸ºæ ‡å‡†åŒ–ï¼šç”¨æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ å‡å»æ•°ç»„çš„å‡å€¼ï¼Œç„¶åé™¤ä»¥[[æ ‡å‡†å·®]]ã€‚
å¦‚æœä½ ä¸ç†Ÿæ‚‰å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç°åœ¨ä¸ç”¨æ‹…å¿ƒï¼Œå®ƒåŒ…å«åœ¨ä¸‹ä¸€æœŸçš„è¯¾ç¨‹ä¸­ã€‚
%%

Normalization is not compulsory - gradient descent would work without it. But due to different units of `X` and `Y`, the cost function will be much steeper. Then you would need to take a significantly smaller learning rate $\alpha$, and the algorithm will require thousands of iterations to converge instead of a few dozens. Normalization helps to increase the efficiency of the gradient descent algorithm.

%%
æ ‡å‡†åŒ–ä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ¢¯åº¦ä¸‹é™ä¹Ÿèƒ½è¿è¡Œã€‚
ä½†æ˜¯ç”±äº `X` å’Œ `Y` çš„é‡çº²ä¸åŒï¼Œæˆæœ¬å‡½æ•°å°†æ›´åŠ é™¡å³­ã€‚
è¿™æ ·ä½ éœ€è¦é€‰æ‹©æ›´å°çš„å­¦ä¹ ç‡ $\alpha$ï¼Œå¹¶ä¸”ç®—æ³•å°†éœ€è¦æ•°åƒæ¬¡è¿­ä»£æ‰èƒ½æ”¶æ•›ï¼Œè€Œä¸æ˜¯å‡ åæ¬¡ã€‚
æ ‡å‡†åŒ–æœ‰åŠ©äºæé«˜æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ•ˆç‡ã€‚
%%

Normalization is implemented in the following code:

%%
ä¸‹é¢çš„ä»£ç å®ç°äº†æ ‡å‡†åŒ–ï¼š
%%


```python
X_norm = (X - np.mean(X))/np.std(X)
Y_norm = (Y - np.mean(Y))/np.std(Y)
```

Define cost function according to the equation $(1)$:

%%
æ ¹æ®æ–¹ç¨‹ $(1)$ å®šä¹‰æˆæœ¬å‡½æ•°ï¼š
%%


```python
def E(m, b, X, Y):
    return 1/(2*len(Y))*np.sum((m*X + b - Y)**2)
```

<a name='ex05'></a>
### Exercise 5


Define functions `dEdm` and `dEdb` to calculate partial derivatives according to the equations $(2)$. This can be done using vector form of the input data `X` and `Y`.

%%
æ ¹æ®æ–¹ç¨‹ $(2)$ å®šä¹‰å‡½æ•° `dEdm` å’Œ `dEdb` è®¡ç®—åå¯¼æ•°ã€‚
è¿™å¯ä»¥åˆ©ç”¨è¾“å…¥æ•°æ® X å’Œ Y çš„å‘é‡å½¢å¼æ¥å®ç°ã€‚
%%


```python
def dEdm(m, b, X, Y):
    ### START CODE HERE ### (~ 1 line of code)
    # Use the following line as a hint, replacing all None.
    res = 1/len(X)*np.dot(m*X + b - Y, X)
    ### END CODE HERE ###

    return res


def dEdb(m, b, X, Y):
    ### START CODE HERE ### (~ 1 line of code)
    # Replace None writing the required expression fully.
    res = 1/len(X) * np.sum(m*X + b - Y)
    ### END CODE HERE ###

    return res

```


```python
print(dEdm(0, 0, X_norm, Y_norm))
print(dEdb(0, 0, X_norm, Y_norm))
print(dEdm(1, 5, X_norm, Y_norm))
print(dEdb(1, 5, X_norm, Y_norm))
```

    -0.7822244248616065
    1.687538997430238e-16
    0.21777557513839416
    5.000000000000001


##### __Expected Output__ 

```Python
-0.7822244248616067
5.098005351200641e-16
0.21777557513839355
5.000000000000002
```

<a name='ex06'></a>
### Exercise 6


Implement gradient descent using expressions $(3)$:

%%
å®ç°æ¢¯åº¦ä¸‹é™çš„è¡¨è¾¾å¼ $(3)$
%%

\begin{align}
m &= m - \alpha \frac{\partial E }{ \partial m },\\
b &= b - \alpha \frac{\partial E }{ \partial b },
\end{align}

where $\alpha$ is the `learning_rate`.


```python
def gradient_descent(dEdm, dEdb, m, b, X, Y, learning_rate = 0.001, num_iterations = 1000, print_cost=False):
    for iteration in range(num_iterations):
        ### START CODE HERE ### (~ 2 lines of code)
        m_new = m - learning_rate * dEdm(m, b, X, Y)
        b_new = b - learning_rate * dEdb(m, b, X, Y)
        ### END CODE HERE ###
        m = m_new
        b = b_new
        if print_cost:
            print (f"Cost after iteration {iteration}: {E(m, b, X, Y)}")
        
    return m, b
```


```python
print(gradient_descent(dEdm, dEdb, 0, 0, X_norm, Y_norm))
print(gradient_descent(dEdm, dEdb, 1, 5, X_norm, Y_norm, learning_rate = 0.01, num_iterations = 10))
```

    (np.float64(0.49460408269589484), np.float64(-1.367128632523413e-16))
    (np.float64(0.9791767513915026), np.float64(4.521910375044022))


##### __Expected Output__ 

```Python
(0.49460408269589495, -3.489285249624889e-16)
(0.9791767513915026, 4.521910375044022)
```


```python
w2_unittest.test_gradient_descent(gradient_descent, dEdm, dEdb, X_norm, Y_norm)
```

    [92m All tests passed


Now run the gradient descent method starting from the initial point $\left(m_0, b_0\right)=\left(0, 0\right)$.

%%
ç°åœ¨å°†åˆå§‹ç‚¹è®¾ç½®ä¸º $\left(m_0, b_0\right)=\left(0, 0\right)$ï¼Œç„¶åå¼€å§‹è¿è¡Œæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼š
%%


```python
m_initial = 0; b_initial = 0; num_iterations = 30; learning_rate = 1.2
m_gd, b_gd = gradient_descent(dEdm, dEdb, m_initial, b_initial, 
                              X_norm, Y_norm, learning_rate, num_iterations, print_cost=True)

print(f"Gradient descent result: m_min, b_min = {m_gd}, {b_gd}") 
```

    Cost after iteration 0: 0.2062999755919659
    Cost after iteration 1: 0.19455197461564455
    Cost after iteration 2: 0.19408205457659172
    Cost after iteration 3: 0.19406325777502959
    Cost after iteration 4: 0.1940625059029671
    Cost after iteration 5: 0.1940624758280846
    Cost after iteration 6: 0.1940624746250893
    Cost after iteration 7: 0.19406247457696948
    Cost after iteration 8: 0.19406247457504472
    Cost after iteration 9: 0.19406247457496772
    Cost after iteration 10: 0.19406247457496462
    Cost after iteration 11: 0.19406247457496453
    Cost after iteration 12: 0.19406247457496453
    Cost after iteration 13: 0.19406247457496448
    Cost after iteration 14: 0.19406247457496448
    Cost after iteration 15: 0.19406247457496448
    Cost after iteration 16: 0.19406247457496448
    Cost after iteration 17: 0.19406247457496448
    Cost after iteration 18: 0.19406247457496448
    Cost after iteration 19: 0.19406247457496448
    Cost after iteration 20: 0.19406247457496448
    Cost after iteration 21: 0.19406247457496448
    Cost after iteration 22: 0.19406247457496448
    Cost after iteration 23: 0.19406247457496448
    Cost after iteration 24: 0.19406247457496448
    Cost after iteration 25: 0.19406247457496448
    Cost after iteration 26: 0.19406247457496448
    Cost after iteration 27: 0.19406247457496448
    Cost after iteration 28: 0.19406247457496448
    Cost after iteration 29: 0.19406247457496448
    Gradient descent result: m_min, b_min = 0.7822244248616065, -3.19744231092045e-16


Remember, that the initial datasets were normalized. To make the predictions, you need to normalize `X_pred` array, calculate `Y_pred` with the linear regression coefficients `m_gd`, `b_gd` and then **denormalize** the result (perform the reverse process of normalization):

%%
è¯·è®°ä½ï¼Œåˆå§‹æ•°æ®é›†éœ€è¦åšæ ‡å‡†åŒ–ã€‚
ä¸ºäº†è¿›è¡Œé¢„æµ‹ï¼Œæ‚¨éœ€è¦å¯¹ `X_pred` æ•°ç»„è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿ç”¨çº¿æ€§å›å½’ç³»æ•° `m_gd` å’Œ `b_gd` è®¡ç®— `Y_pred`ï¼Œç„¶åå¯¹ç»“æœè¿›è¡Œåå½’ä¸€åŒ–ï¼ˆå³æ‰§è¡Œå½’ä¸€åŒ–çš„é€†è¿‡ç¨‹ï¼‰ï¼š
%%


```python
X_pred = np.array([50, 120, 280])
# Use the same mean and standard deviation of the original training array X
X_pred_norm = (X_pred - np.mean(X))/np.std(X)
Y_pred_gd_norm = m_gd * X_pred_norm + b_gd
# Use the same mean and standard deviation of the original training array Y
Y_pred_gd = Y_pred_gd_norm * np.std(Y) + np.mean(Y)

print(f"TV marketing expenses:\n{X_pred}")
print(f"Predictions of sales using Scikit_Learn linear regression:\n{Y_pred_sklearn.T}")
print(f"Predictions of sales using Gradient Descent:\n{Y_pred_gd}")
```

    TV marketing expenses:
    [ 50 120 280]
    Predictions of sales using Scikit_Learn linear regression:
    [[ 9.40942557 12.7369904  20.34285287]]
    Predictions of sales using Gradient Descent:
    [ 9.40942557 12.7369904  20.34285287]


You should have gotten similar results as in the previous sections. 

%%
è¿è¡Œä¸Šé¢çš„ä»£ç åï¼Œä½ åº”è¯¥å·²ç»å¾—åˆ°äº†ä¸å‰é¢ç« èŠ‚ç›¸ä¼¼çš„ç»“æœã€‚
%%

Well done! Now you know how gradient descent algorithm can be applied to train a real model. Re-producing results manually for a simple case should give you extra confidence that you understand what happends under the hood of commonly used functions.

%%
å¹²å¾—æ¼‚äº®ï¼
ç°åœ¨ä½ çŸ¥é“å¦‚ä½•å°†æ¢¯åº¦ä¸‹é™ç®—æ³•è®­ç»ƒçœŸå®çš„æ¨¡å‹ã€‚
é’ˆå¯¹ç®€å•æ¡ˆä¾‹æ‰‹åŠ¨å¤ç°ç»“æœï¼Œåº”è¯¥å¯ä»¥è®©ä½ æ›´æ·±å…¥åœ°ç†è§£å¸¸ç”¨å‡½æ•°çš„å†…éƒ¨è¿ä½œåŸç†ï¼Œä»è€Œè·å¾—é¢å¤–çš„ä¿¡å¿ƒã€‚
%%
